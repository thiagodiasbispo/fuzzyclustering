{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUlhNVAObqE5"
   },
   "source": [
    "# Implementação do algoritimo completo, sem compromisso com  perfomance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import operator\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import fuzzy\n",
    "import clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXxVfg2r6YrR"
   },
   "source": [
    "# Função match\n",
    "\n",
    "![Match function](./img/match_function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kG6AUH5q6lAv"
   },
   "outputs": [],
   "source": [
    "# %load -s cluster_matching_function 'fuzzy.py'\n",
    "def cluster_matching_function(weight_matrix,\n",
    "                              cluster_number,\n",
    "                              element,\n",
    "                              prototypes,\n",
    "                              dissimilarity_matrices):\n",
    "    \"\"\"\n",
    "        :params: weight_matrix: numpy array-like \n",
    "                    matriz K x P de pesos das matrizes de dissimilaridades por cluster\n",
    "                cluster_number: int\n",
    "                    Número do cluster em questão\n",
    "                element: int\n",
    "                    Índice do elemento (entre 0 e N-1)\n",
    "                prototypes: list like\n",
    "                    Lista de tamanho K dos protótipos de cada cluster\n",
    "                dissimilarity_matrices: lista de numpy array\n",
    "                    Lista de matrizes de dissimilaridade\n",
    "\n",
    "        :return: float\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Criando aliases compatíveis com as variáveis da fórmula\n",
    "    k = cluster_number\n",
    "    D = dissimilarity_matrices\n",
    "    p = len(D)\n",
    "    Gk = prototypes[k]\n",
    "    l = weight_matrix\n",
    "\n",
    "    dissimilarities_sum = np.array([dj[element, Gk].sum() for dj in D])\n",
    "\n",
    "    return np.dot(l[k], dissimilarities_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EdvnqRHK8wBq"
   },
   "source": [
    "# Função objetivo\n",
    "\n",
    "![Objetive function](./img/objective_function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CUfSEUkfcAnN"
   },
   "outputs": [],
   "source": [
    "# %load -s objective_function 'fuzzy.py'\n",
    "def objective_function(clusters_qtd,\n",
    "                       elements_qtd,\n",
    "                       adequacy_criterion,\n",
    "                       m,\n",
    "                       weight_matrix,\n",
    "                       prototypes,\n",
    "                       dissimilarity_matrices):\n",
    "    \"\"\"\n",
    "        :params: clusters_qtd: int\n",
    "                    Quantidade total de clusters\n",
    "                elements_qtd: int\n",
    "                    Quantidade de elementos da base de dados\n",
    "                adequacy_criterion: numpy array-like\n",
    "                    Matriz u de tamanho N x K contendo a índice de adequação \n",
    "                    de cada elemente a cada cluster\n",
    "                m: int\n",
    "                    Fator de ponderação do índice de adequação\n",
    "                weight_matrix:\n",
    "                     matriz K x P de pesos das matrizes de dissimilaridades por cluster\n",
    "                prototypes: list like\n",
    "                    Lista de tamanho K dos protótipos de cada cluster\n",
    "                dissimilarity_matrices: lista de numpy array\n",
    "                    Lista de matrizes de dissimilaridade\n",
    "\n",
    "        :return: float\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    u = np.power(adequacy_criterion, m) # Resolvendo a exponeciação de u de uma vez só\n",
    "    l = weight_matrix\n",
    "    D = dissimilarity_matrices\n",
    "    K = clusters_qtd\n",
    "    G = prototypes\n",
    "    N = elements_qtd\n",
    "    match = cluster_matching_function # Criando um alias para reduzir o nome da função de matching\n",
    "  \n",
    "    J = np.array([np.array([u[i, k] * match(l, k, i, G, D) for i in range(N)]).sum() \n",
    "          for k in range(K)])\n",
    "\n",
    "\n",
    "    return J.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MzyvZQxb9Iiy"
   },
   "source": [
    "# Protótipos\n",
    "\n",
    "![Prototype function](./img/prototype_function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s get_prototypes 'fuzzy.py'\n",
    "\n",
    "def get_prototypes(elements_qtd,\n",
    "                       q,\n",
    "                       m,\n",
    "                       s,\n",
    "                       cluster_number,\n",
    "                       adequacy_criterion,\n",
    "                       dissimilarity_matrices,\n",
    "                       weight_matrix):\n",
    "    G = []\n",
    "    k = cluster_number\n",
    "    D = dissimilarity_matrices\n",
    "    u = np.power(adequacy_criterion, m)\n",
    "    l = np.power(weight_matrix, s)\n",
    "    N = elements_qtd\n",
    "    P = len(D)\n",
    "    \n",
    "    while (len(G) != q):\n",
    "        menor_soma = 999999\n",
    "        menor_indice = None\n",
    "        \n",
    "        for h in range(N): \n",
    "            if h in G:\n",
    "                continue\n",
    "            \n",
    "            dists_p = np.array([D[j][:, h] * l[k,j] for j in range(P)]) #shape: NxP\n",
    "            sums_p = dists_p.sum(axis=0)\n",
    "            soma = np.dot(u[:, k], sums_p)\n",
    "\n",
    "            if soma < menor_soma:\n",
    "                menor_soma = soma\n",
    "                menor_indice = h\n",
    "                 \n",
    "        G.append(menor_indice)\n",
    "        \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmu24Eka9wdS"
   },
   "source": [
    "# Matriz de relevâcia\n",
    "\n",
    "![Funções de peso](./img/vector_weights_function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02tqmGYnBImF"
   },
   "outputs": [],
   "source": [
    "# %load -s compute_relevance_weights 'fuzzy.py'\n",
    "def compute_relevance_weights(clusters_qtd,\n",
    "                              dissimilarity_matrices,\n",
    "                              prototypes,\n",
    "                              elements_qtd,\n",
    "                              adequacy_criterion,\n",
    "                              m):\n",
    "    \"\"\"\n",
    "        :params:\n",
    "                clusters_qtd: int\n",
    "                    Quantidade total de clusters\n",
    "                dissimilarity_matrices: lista de numpy array\n",
    "                    Lista de matrizes de dissimilaridade\n",
    "                prototypes: list like\n",
    "                    Lista de tamanho K dos protótipos de cada cluster\n",
    "                elements_qtd: int\n",
    "                    Quantidade de elementos da base de dados\n",
    "                adequacy_criterion: numpy array-like\n",
    "                    Matriz u de tamanho N x K contendo a índice de adequação \n",
    "                    de cada elemente a cada cluster\n",
    "                m: int\n",
    "                    Fator de ponderação do índice de adequação\n",
    "\n",
    "        :return: numpy array of shape K x P\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    D = dissimilarity_matrices\n",
    "    P = len(D)\n",
    "    G = prototypes\n",
    "    K = clusters_qtd\n",
    "    N = elements_qtd\n",
    "    u = np.power(adequacy_criterion, m)\n",
    "    l = np.zeros((K, P))\n",
    "\n",
    "    def match(element, Dh, Gk):\n",
    "        \"\"\"\n",
    "            Função auxiliar para cálculo de match entre um elemento \n",
    "            qualquer, os protótipos G de um cluster específico e uma matriz \n",
    "            de similaridade específica Dh.\n",
    "        \"\"\"\n",
    "\n",
    "        return Dh[element, Gk].sum()\n",
    "\n",
    "    for k in range(K):\n",
    "        # Calculado o somatório do numerador da equação à esquerda da igualdade\n",
    "        weight_diss_sum1 = np.array([np.array([u[i, k] * match(i, D[h], G[k]) for i in range(N)]).sum()\n",
    "                            for h in range(P)])\n",
    "\n",
    "        weight_diss_sum_prod = np.power(weight_diss_sum1.prod(), 1/P)\n",
    "\n",
    "        for j in range(P):\n",
    "     \n",
    "            # Calculado o somatório do denominador da equação à esquerda da igualdade\n",
    "            weight_diss_sum2 = np.array([u[i, k] * match(i, D[j], G[k])\n",
    "                                    for i in range(N)]).sum()\n",
    "            \n",
    "\n",
    "            # Executando a divisão da fração à esquerda da equação\n",
    "            l[k, j] = weight_diss_sum_prod / weight_diss_sum2\n",
    "\n",
    "    return l\n",
    "\n",
    "\n",
    "# %load -s compute_membership_degree 'fuzzy.py'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grau de pertinência\n",
    "\n",
    "![Fórmula grau de pertinência](./img/membership_degree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s compute_membership_degree 'fuzzy.py'\n",
    "def compute_membership_degree(weight_matrix,\n",
    "                              prototypes,\n",
    "                              clusters_qtd,\n",
    "                              dissimilarity_matrices,\n",
    "                              elements_qtd,\n",
    "                              m):\n",
    "    \"\"\"\n",
    "        :params: weight_matrix: numpy array-like \n",
    "                    matriz K x P de pesos das matrizes de dissimilaridades por cluster\n",
    "                prototypes: list like\n",
    "                    Lista de tamanho K dos protótipos de cada cluster\n",
    "                clusters_qtd: int\n",
    "                    Quantidade total de clusters\n",
    "                dissimilarity_matrices: lista de numpy array\n",
    "                    Lista de matrizes de dissimilaridade\n",
    "                elements_qtd: int\n",
    "                    Quantidade de elementos da base de dados\n",
    "                m: int\n",
    "                    Fator de ponderação do índice de adequação\n",
    "\n",
    "        :return: numpy array NxK\n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "\n",
    "    K = clusters_qtd\n",
    "    G = prototypes\n",
    "    D = dissimilarity_matrices\n",
    "    l = weight_matrix\n",
    "    P = len(D)\n",
    "    N = elements_qtd\n",
    "    u = np.zeros((N, K))\n",
    "    \n",
    "    match = cluster_matching_function # Criando um alias para reduzir o nome da função de matching\n",
    "\n",
    "    def ratio(element, k, h):\n",
    "        r = match(l, k, element, G, D) / match(l, h, element, G, D)\n",
    "        #r1 = np.array([l[k,j] * (D[j][element, G[k]].sum()) for j in range(P)]).sum()\n",
    "        #r2 = np.array([l[h,j] * (D[j][element, G[h]].sum()) for j in range(P)]).sum()\n",
    "        #r3 = r1/r2\n",
    "        #print(f\"Ratio: {r} {r3}\")\n",
    "        return np.power(r, 1/(m-1))\n",
    "\n",
    "    for i in range(N):\n",
    "        for k in range(K):\n",
    "            outter_sum = np.array([ratio(i, k, h) for h in range(K)]).sum()\n",
    "            u[i, k] = 1/outter_sum\n",
    "\n",
    "    return u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SS-G2leHdobn"
   },
   "source": [
    "# Carregando Matrizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2gEvW3EdWJT"
   },
   "outputs": [],
   "source": [
    "fac_dis, fou_dis, kar_dis = fuzzy.carregar_matrizes_dissimiliradidades()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo completo\n",
    "> Partitioning fuzzy K-medoids clustering algorithms with relevance weight for each dissimilarity matrix estimated locally\n",
    "\n",
    "* Parametros: $K = 10; m = 1.6; T = 150; \\epsilon = 10^{−10};$\n",
    "* Devemos considerar a iniciarlizar do vetor de pesos como sendo 1, já que usamos a equação 9 (MFCMdd-RWL-P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s random_prototypes,executar_treinamento 'fuzzy.py'\n",
    "def random_prototypes(K, N, q, seed):\n",
    "    elements = tuple(range(N))\n",
    "    protos = []\n",
    "    random.seed(seed)\n",
    "    \n",
    "    for k in range(K):\n",
    "        protos.append(random.sample(elements, q))\n",
    "\n",
    "    return protos\n",
    "\n",
    "def executar_treinamento(dissimilarity_matrices,\n",
    "                       elements_qtd,\n",
    "                       K=10,\n",
    "                       m=1.6,\n",
    "                       T=150,\n",
    "                       epsilon=10e-10,\n",
    "                       q=2, \n",
    "                       seed=13082020,\n",
    "                       prototipos = None):\n",
    "\n",
    "    D = dissimilarity_matrices\n",
    "    N = elements_qtd\n",
    "    P = len(D)\n",
    "\n",
    "    last_lambda = np.ones((K, P))\n",
    "    last_prototypes = prototipos or random_prototypes(K, N, q, seed)\n",
    "    last_membership_degree = None\n",
    "    last_cost = None\n",
    "    \n",
    "    #assert_relevance_weights_prod_one(last_lambda)\n",
    "\n",
    "#     print(\"Passo 0\")\n",
    "#     print(\"Calculando matriz de adequação inicial (u0)\")\n",
    "    u0 = compute_membership_degree(weight_matrix=last_lambda,\n",
    "                                   prototypes=last_prototypes,\n",
    "                                   clusters_qtd=K,\n",
    "                                   dissimilarity_matrices=dissimilarity_matrices,\n",
    "                                   elements_qtd=N,\n",
    "                                   m=m)\n",
    "    \n",
    "#     assert_membership_degree_sum_one(u0)\n",
    "\n",
    "#     print(\"Calculando função de custo inicial (J0)\")\n",
    "    J0 = objective_function(clusters_qtd=K,\n",
    "                            elements_qtd=N,\n",
    "                            adequacy_criterion=u0,\n",
    "                            m=m,\n",
    "                            weight_matrix=last_lambda,\n",
    "                            prototypes=last_prototypes,\n",
    "                            dissimilarity_matrices=dissimilarity_matrices)\n",
    "    \n",
    "    last_membership_degree = u0\n",
    "    last_cost = J0\n",
    "    first_prototypes = last_prototypes\n",
    "    for t in range(1, T):\n",
    "#         print(f\"Passo {t}/{T}\")\n",
    "        \n",
    "#         print(\">> Calculando protótipos\")\n",
    "        new_prototypes = [get_prototypes(elements_qtd=N,\n",
    "                                         q=q,\n",
    "                                         m=m,\n",
    "                                         s=1,\n",
    "                                         cluster_number=k,\n",
    "                                         adequacy_criterion=last_membership_degree,\n",
    "                                         dissimilarity_matrices=D,\n",
    "                                         weight_matrix=last_lambda) for k in range(K)]\n",
    "        \n",
    "        #print(\"new_prototypes.shape\", new_prototypes)\n",
    "        \n",
    "#         print(\">> Calculando matriz de relevâncias\")\n",
    "        new_lambda = compute_relevance_weights(clusters_qtd=K,\n",
    "                                               dissimilarity_matrices=D,\n",
    "                                               prototypes=new_prototypes,\n",
    "                                               elements_qtd=N,\n",
    "                                               adequacy_criterion=last_membership_degree,\n",
    "                                               m=m)\n",
    "        \n",
    "#         assert_relevance_weights_prod_one(new_lambda)\n",
    "    \n",
    "#         print(\">> Calculando grau de pertinência\")\n",
    "        new_degree = compute_membership_degree(weight_matrix=new_lambda,\n",
    "                                               prototypes=new_prototypes,\n",
    "                                               clusters_qtd=K,\n",
    "                                               dissimilarity_matrices=dissimilarity_matrices,\n",
    "                                               elements_qtd=N,\n",
    "                                               m=m)\n",
    "    \n",
    "        \n",
    "#         assert_membership_degree_sum_one(new_degree)\n",
    "\n",
    "#         print(\">> Calculando função objetivo\")\n",
    "        new_cost = objective_function(clusters_qtd=K,\n",
    "                                      elements_qtd=N,\n",
    "                                      adequacy_criterion=new_degree,\n",
    "                                      m=m,\n",
    "                                      weight_matrix=new_lambda,\n",
    "                                      prototypes=new_prototypes,\n",
    "                                      dissimilarity_matrices=dissimilarity_matrices)\n",
    "\n",
    "        last_prototypes = new_prototypes\n",
    "        last_lambda = new_lambda\n",
    "        last_membership_degree = new_degree\n",
    "        print(f\">> Cost ({seed}): \", new_cost)\n",
    "        \n",
    "        if abs(last_cost - new_cost) <= epsilon:\n",
    "            last_cost = new_cost\n",
    "            break\n",
    "    \n",
    "        last_cost = new_cost\n",
    "        \n",
    "    data = {\n",
    "        \"cost\":last_cost,\n",
    "        \"membership_degree\":last_membership_degree,\n",
    "        \"first_prototypes\": first_prototypes,\n",
    "        \"last_prototypes\":last_prototypes,\n",
    "        \"weight_matrix\":last_lambda,\n",
    "        \"times\": t,\n",
    "        \"q\": q,\n",
    "        \"K\":K,\n",
    "        \"m\":m,\n",
    "        \"seed\": seed,\n",
    "    }\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executando 100x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzy\n",
    "melhor_resultado_todas = fuzzy.executar_algoritmo_varias_vezes([fac_dis, fou_dis, kar_dis], \n",
    "                                                                2000, \n",
    "                                                                m=1.6,\n",
    "                                                                report_file = \"data/relatorio_varias_execucoes_todas.csv\",\n",
    "                                                                q=2,\n",
    "                                                                times=1)\n",
    "\n",
    "fuzzy.export_best_result(melhor_resultado_todas, \"data/melhor_resultado_todas.pickle\")\n",
    "fuzzy.export_fuzzy_partitions_to_csv(melhor_resultado_todas, \"data/fuzzy_partitions_todas.csv\")\n",
    "\n",
    "melhor_resultado_todas[\"cost\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Cost (18082020):  3407.3654578130063\n",
      ">> Cost (18082020):  3391.711503077616\n",
      ">> Cost (18082020):  3391.711503077616\n",
      "Execução 1/1\n",
      ">> Cost:  3391.711503077616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3391.711503077616"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melhor_resultado_fac = fuzzy.executar_algoritmo_varias_vezes([fac_dis], 2000, times=100)\n",
    "fuzzy.export_best_result(melhor_resultado_fac, \"data/melhor_resultado_fac.pickle\")\n",
    "fuzzy.export_fuzzy_partitions_to_csv(melhor_resultado_fac, \"data/fuzzy_partitions_fac.csv\")\n",
    "\n",
    "melhor_resultado_fac[\"cost\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Cost (18082020):  1802.4071473810295\n",
      ">> Cost (18082020):  1800.7496330783467\n",
      ">> Cost (18082020):  1800.7496330783467\n",
      "Execução 1/1\n",
      ">> Cost:  1800.7496330783467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1800.7496330783467"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melhor_resultado_fou = fuzzy.executar_algoritmo_varias_vezes([fou_dis], 2000, times=100)\n",
    "fuzzy.export_best_result(melhor_resultado_fou, \"data/melhor_resultado_fou.pickle\")\n",
    "fuzzy.export_fuzzy_partitions_to_csv(melhor_resultado_fou, \"data/fuzzy_partitions_fou.csv\")\n",
    "\n",
    "melhor_resultado_fou[\"cost\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Cost (18082020):  1518.7438733671443\n",
      ">> Cost (18082020):  1518.7438733671443\n",
      "Execução 1/1\n",
      ">> Cost:  1518.7438733671443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1518.7438733671443"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melhor_resultado_kar = fuzzy.executar_algoritmo_varias_vezes([kar_dis], 2000, times=100)\n",
    "fuzzy.export_best_result(melhor_resultado_kar, \"data/melhor_resultado_kar.pickle\")\n",
    "fuzzy.export_fuzzy_partitions_to_csv(melhor_resultado_kar, \"data/fuzzy_partitions_kar.csv\")\n",
    "\n",
    "melhor_resultado_kar[\"cost\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterinando melhores parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# qs = list(range(2, 6))\n",
    "# ms = np.arange(1., 2.1, .1)\n",
    "\n",
    "# fuzzy.buscar_melhores_parametros(qs, ms, [fac_dis, fou_dis, kar_dis])\n",
    "# fuzzy.buscar_melhores_parametros(qs, ms, [fac_dis], file_name=\"data/melhores_parametros_fac.csv\"), times= 10)\n",
    "# fuzzy.buscar_melhores_parametros(qs, ms, [fou_dis], file_name=\"data/melhores_parametros_fou.csv\"), times = 10)\n",
    "# fuzzy.buscar_melhores_parametros(qs, ms, [kar_dis], file_name=\"data/melhores_parametros_kar.csv\"), times = 10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02_clusterizar_lazy_version.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
